# =======================================================================================
#       MINKA
#       Distributor, Sharding of duties
#
#       These are the default values fixed if no override specified
#       See source file: Config 
# =======================================================================================

# service name to identify within Minka (uses a zookeeper namespace 
# for each service), multiples services within same ZK ensemble are allowed
serviceName=anyName

# zookeeper string to pass to driver
zookeeperHostPort=localhost:2181
# InetAddress: empty to use site IP address
brokerServerHost=127.0.0.1
# int: communication TCP port between leaders and followers
brokerServerPort=17017
# int: concurrent handled connections when leader, > followers size
brokerServerConnectionHandlerThreads=5
# int: both follower and leader have servers and clients that connect to other broker's servers
brokerRetryDelayMs=3000
brokerMaxRetries=10
# long: milliseconds to wait between tries to boot minka
# to let the PartitionDelegate get together and be ready for distribution
bootstrapReadynessRetryDelayMs=5000
	
# bool: not all shards might want to candiate for leader 
# but if you restrain this your chances of HA go down, and Minka doesnt work without Leader
bootstrapPublishLeaderCandidature=true
   
# bool: where the leader is elected also run a follower to process duties
bootstrapLeaderShardAlsoFollows=true

# long: milliseconds = 5 mins default retaining messages from followers to leader's partition
queuePartitionRetentionLapseMs=300000
queueInboxRetentionLapseMs=300000
queueUserRetentionLapseMs=300000

# =======================================================================================
# Follower shard params

# string: a network interfase name with a site address (no link, no loopback)
# such interfase must have a unique host name 
followerUseNetworkInterfase=eth0
# string: suffix for the shard id 
# to enable multiple followers within the same host for the same "service"
followerShardIDSuffix=1

# long: milliseconnds, send a heartbeat to notify the shard is alive = each half second 
followerHeartbeatStartDelayMs=1000
followerHeartbeatDelayMs=100

# long: milliseconds, 10 seconds enough to start check and release duties if no HB in x time 
followerHeartattackCheckStartDelayMs=10000
followerHeartattackCheckDelayMs=1000

# 10 seconds absence to instruct delegate to release duties
followerMaxHeartbeatAbsenceForReleaseMs=10000

# int: 10 errors tolerant for building HBs from followers */ 
followerMaxHeartbeatBuildFailsBeforeReleasing=10

# int: milliseconds, shoot the clearance check 20 seconds after bootstrap
# then each 10 seconds
followerClearanceCheckStartDelayMs=20000
followerClearanceCheckDelayMs=10000
# int: milliseconds, max old for last clearance received before releasing all duties
followerClearanceMaxAbsenceMs=30000

# =======================================================================================
# Leader: Distributor and balancing of duties

# bool: false to let the client pass the initial duties to distribute (see PartitionMaster), 
# true to let Minka handle storage for this (see PartitionDelegate)
minkaStorageDuties=false

# bool: check for absent duties each time the distributor runs (log only)
distributorRunConsistencyCheck=true
# bool: facility to avoid mandatory CRUD operations thru PartitionService
distributorReloadDutiesFromStorage=false
# int: each period the distributor runs
distributorReloadDutiesFromStorageEachPeriods=10

# 10 seconds to let the Shepherd discover all Followers before Distributing
# long: milliseconds distributor start 
distributorStartDelayMs=10000
distributorDelayMs=5000

# int: max retries executing the same reallocation before starting over an analysis
distributorReallocationMaxRetries=2
# int: seconds, time to expire reallocations and start over a distributor analysis
distributorReallocationExpirationSec=15


# BalanceStrategy: each strategy is implemented on a different balancer behaviour
# EVEN_SIZE = all shard same amount of duties
# FAIR_LOAD = duties clustering according weights
# SPILL_OVER = fill each shard until spill, then fill another one, so on
distributorBalancerStrategy=FAIR_LOAD

# int: only for EVEN balance strategy: max diff between more or less 
# duties to tolerate before instruct the shard to release or take Duty
balancerEvenSizeMaxdutiesDeltaBetweenShards=1

# FairBalancerPreSort: criteria for presorting the duties collection before clustering
# DATE 
# Use Creation date order, i.e. natural order.
# Use this to keep the migration of duties among shards: to a bare minimum.
# Duty workload weight is considered but natural order restricts the re-accomodation much more.
# Useful when the master list of duties has few changes, and low migration is required. 
#
# WORKLOAD 
# Use Workload order.
# Use this to maximize the clustering algorithm's effectiveness.
# In presence of frequent variation of workloads, duties will tend to migrate more. 
balancerFairLoadPresort=WORKLOAD

# SpillBalancerStrategy: [only for SPILL balancer] how to use the max value set 
# in comparing the sum of all workloads or the sum running duties
# WORKLOAD 
# Use the Max value to compare the sum of all running duties's weights 
# and restrict new additions over a full shard 
#
# SIZE 
# Use the Max value as max number of duties to fit in one shard  
balancerSpillOverStrategy=WORKLOAD

# long: max value used to fit more or less duties within 1 shard before hopping to the next
# 0 to disable balancing and keep high availability
balancerSpillOverMaxValue=0

# =======================================================================================
# Leader: analysis of Shards's status

# long: milliseconds, each 3 seconds, scheduler for the Shepherd process 
# that analyzes shards thru their heartbeats and writes the PartitionTable 
# by setting ONLINE/OFFLINE any given shard
shepherdStartDelayMs=1000
shepherdDelayMs=3000

# int: min healthly heartbeats to consider before passing a shard from Quarantine to Online
shepherdMinHealthlyHeartbeatsForShardOnline=2 
# int: min absent heartbeats before puting the shard Offline and redistributing its duties
shepherdMinAbsentHeartbeatsBeforeShardGone=20
# double: 
shepherdMaxHeartbeatReceptionDelayFactorForSick=0.5d
# int: min unhealthly heartbeats before moving the shard from Online to Quarantine
shepherdMinSickHeartbeatsBeforeShardQuarantine=10
# int: min shards required to start distributing duties
shepherdMinShardsOnlineBeforeSharding=1
# double: max time lapse between heartbeats measured in a standard deviation statistic
shepherdHeartbeatMaxBiggestDistanceFactor=1.5d
# int: max seconds to evaluate for the shard's timewindow of heartbeats 
shepherdHeartbeatLapseSec=30
# double: max time lapse between heartbeats measured in a standard deviation statistic
shepherdHeartbeatMaxDistanceStandardDeviation=3
